---
title: "grsearch_rmf"
author: "Haozhi Ma"
date: "5/7/2020"
output: html_document
---


This script is used for an offline grid search to find the best models that could fit forest training dataset. Thus we could find the best fit hyperparameters that we can use for mapping.

The aim of doing grid search is to try the best avoiding overfitting, which most critics on machine learning have focused on. Higher R2 here, computed as coefficient of determining variation, means lower RMSE. This R2 or RMSE, is commonly used to detect whether the model is good or not in terms of avoiding overfitting.

The R version is 3.5.1 with h2o package 3.32.0.1 and data.table package 1.12.8. Before running this script, make sure you have equal or higher than thses versions for your R and packages. 

If the dataset's directory is set, just use ctrl+alt+R to run all the chunks from the begining till the end. You can both have the model performance and a scatter plot. Also, this script outputs the best ten models that will be used in mapping approach and their performace. To compute the ensemble approach performace, a R2 is also calculated. 


Load the package
```{R}
library(data.table)
library(h2o)
library(raster)
library(tictoc)
library(foreach)
library(doParallel)
library(tidyverse)
```
Load the dataset. Here I put forest dataset as an example. Same procedure can be replaced by grassland or shrubland datset. 

```{R}
forest.rsr.grsearch<-read.csv('Forest training dataset.csv')

```

Check the basic structure of the dataset.
```{R}
names(forest.rsr.grsearch)
median(forest.rsr.grsearch$ratio)
mad(forest.rsr.grsearch$ratio)
mean(forest.rsr.grsearch$ratio)
```


Select bands.

```{R}
bands<-subset(forest.rsr.grsearch, select = -c(system.index,latitude,longitude,index,Pixel_Lat, Pixel_Long,latitude_209564535,longitude_209564535,ratio,.geo,Global_Biomass_IPCC,X,X_1,Abs_Lat,rmf))
bandnames<-names(bands)
print(bandnames)

```


```{R}
rawmatrix<-forest.rsr.grsearch

```



set the name of the dependent variable

```{R}
vartomodel<-'rmf'
```
select the bands from the regression matrix
```{R}
regressionmatrix<-rawmatrix %>%
  select(bandnames, vartomodel)
```

Initialize the local environment. All the parameters here should fit your computer settings.
```{R}
localH2O<-h2o.init(nthreads = 7, max_mem_size = '10g', ignore_config = TRUE)

```


import the regression matrix
```{R}
regmatrixh2o<-as.h2o(regressionmatrix, destination_frame = 'regMatrixH2O')

## Important, without this matrix, you can hardly run grid search.
```


set grsearch parameters

```{R}
rf.params<-list(ntrees = c(50,100,150),
                mtries = c(2:10),
                min_rows = c(2:5)
                #### I would recommend higher ntrees if you would like to have a hihger model performance. For example, ntree = 500. Normally, the higher the value of these parameters, the higher the possible model performance. Do compromise with your computer capacity. 
                )

```

set grsearch criteria

```{R}
search.criteria<-list(strategy = 'RandomDiscrete', max_models = 150, seed = 0, max_runtime_secs = 3600)


```



perform rf grsearch across parameters

Here we should keep cross validation predictions, these predictions will be used for calculating coefficients of determination values. 
```{R}
rf.grid<-h2o.grid('randomForest',
                  y = vartomodel,
                  grid_id = 'rf.grid',
                  training_frame = regmatrixh2o,
                  seed = 0,
                  hyper_params = rf.params,
                  sample_rate = 0.632,
                  nfolds = 10,
                  fold_assignment = 'AUTO',
                  keep_cross_validation_predictions = TRUE, # Important!
                  keep_cross_validation_fold_assignment = TRUE, # Important!
                  search_criteria = search.criteria)
```



retrieve grid searched model performance sort by RMSE/R2


```{R}
rf.grid.perf<-h2o.getGrid(grid_id = 'rf.grid',
                          sort_by = 'R2', # This is also coefficient of determinaion values
                          decreasing = TRUE)

```





```{R}
print(rf.grid.perf@summary_table)

```

Save the best model hyperparameters.
```{R}
write.csv(rf.grid.perf@summary_table,'C:\\Users\\haozh\\Desktop\\root_ratio\\grsearch\\forest_rfgrsearch_r2_20200517.csv')
```

grab the best models,

```{R}

for(i in 1:10){
  bestrf<-h2o.getModel(rf.grid.perf@model_ids[[i]])

h2o.saveModel(object = bestrf, path = 'C:\\Users\\haozh\\Desktop\\root_ratio\\grsearch\\best_forest_models',force = TRUE)

}

```


Here explains why saving cross validation predictions is important. We need to ensemble them across ten different models.

```{R}
bestrf<-h2o.getModel(rf.grid.perf@model_ids[[1]])

h2o.cross_validation_holdout_predictions(bestrf)

```

loop the best model cv predictions


```{R}
cvprediction<-as.data.frame(regressionmatrix$rmf)
for(i in 1:10){
  bestrf<-h2o.getModel(rf.grid.perf@model_ids[[i]])
  
  cvprediction[,i+1]<-as.data.frame(h2o.cross_validation_holdout_predictions(bestrf))
}
head(cvprediction)
```
Average the ten best models. This mean prediction values will be evaluated later.
```{R}
cvprediction$predictmean<-rowMeans(cvprediction[,-1])
head(cvprediction)
```


Save it to a file for further use.
```{R}

write.csv(cvprediction, 'C:\\Users\\haozh\\Desktop\\root_ratio\\grsearch\\forest_cvprediction_20200821.csv')

```


r2 of best performing RF

```{R}
h2o.r2(bestrf, xval = TRUE)

```
variable importance metrics

```{R}
h2o.varimp_plot(bestrf)
h2o.varimp(bestrf)$variable
```

Here is the function that we calculate the coefficient of determination values. 

```{R}
coef_det <- function(xtrue, xpred){
    return(1-sum((xtrue-xpred)^2)/sum((xtrue-mean(xtrue))^2))
}

```
Get the R sqaure. Note here is R sqaure is the comparison of the pairs of empirical rmf and predictions with the curve line "y = x". Here is the measurement of the ensemble approach performance. 
```{R}
coef_det(cvprediction$`regressionmatrix$rmf`,cvprediction$predictmean)

```

Shut down h2o to release your memory. Good for your computer. 
```{R}
h2o.shutdown(prompt = FALSE)

```


define palette

```{R}
paletteForUse <- c('#d10000', '#ff6622', '#ffda21', '#33dd00', '#1133cc', '#220066', '#330044')
colors <-  colorRampPalette(paletteForUse)(256)

cvprediction$dens <- col2rgb(densCols(cvprediction$`regressionmatrix$rmf`, cvprediction$predictmean))[1,] + 1L
cvprediction$colors = colors[cvprediction$dens]



```

Plotting.

```{R}
ggplot(cvprediction, 
                      aes(x = cvprediction$predictmean, 
                          y = cvprediction$`regressionmatrix$rmf`)) +
  geom_point(color = cvprediction$colors) +
  labs(x = "Predicted",
       y = "Observed") +
  #coord_cartesian(xlim = c(0,6), ylim = c(0,6)) +
  stat_smooth(se = T,
              colour = "black",
              size = 0.5,
             method = "lm") +
  geom_abline(slope = 1,
              intercept = 0,
              na.rm = FALSE, 
              show.legend = NA, 
              linetype = "dashed") +
  theme_bw()

```


